---
title: "Prediction Assignment - Practical Machine Learning Course"
author: "Kiril Genov"
date: "July 16, 2018"
output: html_document
---

# Overview
The current document represents the analysis performed with regard to the Prediction Assignment to the Practical Machine Learning Course, taught by Mr. John Caffo.
The goal of the assignment is to predict how well 6 participants do a weight lifting exercise, based on the data from accelerometers on the belt, forearm, arm, and dumbell. Basically that means we have to find a classification algorythm to predict the outcome of an exercise.
More information on how the experiment and how the data was collected can be found on the following link: http://groupware.les.inf.puc-rio.br/har

Our approach to this assignment would be the following:
- cleaning of the original dataset;
- cross validation;
  - split into training/testing datasets
  - applying several models on the training set
  - evaluating: comparing results and accuracy on the testing set
- choosing a model;
- applying the model to the 20 test cases.

# Reading the data into R and some basic analysis
Here's the code for reading the data into R

```{r load_data, echo=TRUE, message=FALSE, warning=FALSE}
url_train<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url_test<-"https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(url = url_train, destfile = "./pml-training.csv")
download.file(url = url_test, destfile = "./pml-testing.csv")
training<-read.csv("./pml-training.csv", na.strings = "NA")
testing<-read.csv("./pml-testing.csv", na.strings = "NA")
```

Using the dim() function we can see how many rows and columns we have in the dataset. The summary(training) function would give us a bit more information. We will not perform it here to spare some space in the document.
```{r dim, echo=TRUE, message=FALSE, warning=FALSE}
dim(training)
```

# Cleaning the data
Some of the columns have a lot of NA values, and some of them have a lot of zeros. We will create a function which will show us which colums have more than 50% of missing and zero values, so that we can discard them from our model.
```{r cleaning, echo=TRUE, message=FALSE, warning=FALSE}
subs<-as.numeric()
out_of_subs<-as.numeric()
for (i in 1:ncol(training)){
  x <- sum(training[,i]=="")
  y <- sum(is.na(training[,i]))
  z <- length(training[,i])
  if (is.na(x) == TRUE){x<-0}
  if ((x+y)/z < 0.5){
    subs[length(subs)+1]<-i
  }
  rm(x,y,z)
}
``` 

We will also remove the first five columns, since they contain personal information or information irrelevant to the model.
```{r cleaning2, echo=TRUE, message=FALSE, warning=FALSE}
subs<-subs[-(1:5)]
```

Finally we subset the original data to get a clean dataset with less columns.
```{r subset, echo=TRUE, message=FALSE, warning=FALSE}
dataset<-training[,subs]
```

# Splitting the original dataset
The next task is to divide the training dataset provided into two subsets (70% and 30%). One of them will be for building our models and the other will be for testing purposes, before we apply the selected model to the 20 cases.
For the purposes of reproducibility of this analysis, we set the seed a particular seed.
```{r split, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(11111)
library(caret)
inTrain <- createDataPartition(y=dataset$classe, p=0.7, list=FALSE)
intraining <- dataset[inTrain,]
intesting <- dataset[-inTrain,]
```

Now we will apply three models to predict the "classe" variable with all the other variables.
The first model will be the Random Forest model.
```{r rf, echo=TRUE, message=FALSE, warning=FALSE}
library(randomForest)
mod_rf<-randomForest(classe~., data = intraining)
```

The second model will be Classification Tree.
```{r rpart, echo=TRUE, message=FALSE, warning=FALSE}
mod_rpart<-train(classe ~ ., data = intraining, method = "rpart")
```

The third model will be Linear Discriminant Analysis.
```{r lda, echo=TRUE, message=FALSE, warning=FALSE}
mod_lda<-train(classe ~ ., data = intraining, method = "lda")
```

And last but not least we will stack the predictions together using Random Forests again to get a combined model
We will perform this by applying the three models to the testing dataset and predict the "classe" variable with the results.
```{r combined, echo=TRUE, message=FALSE, warning=FALSE}
pred_rf<-predict(mod_rf, intesting)
pred_rpart<-predict(mod_rpart, intesting)
pred_lda<-predict(mod_lda, intesting)
predDF<-data.frame(pred_rf, pred_rpart, pred_lda, classe=intesting$classe)
combinedModel<-randomForest(classe~., data=predDF)
pred_combined<-predict(combinedModel, intesting)
```

# Selecting a model
Now that we have all our models set and running, let's compare the results against the actual values.
```{r select, echo=TRUE, message=FALSE, warning=FALSE}
names<-c("Random Forest", "Classification Tree", "Linear Discriminant Analysis", "Combined Model", "Actual Values")
predictions<-data.frame(summary(pred_rf),
                        summary(pred_rpart),
                        summary(pred_lda),
                        summary(pred_combined),
                        summary(intesting$classe))
colnames(predictions)<-names
print(predictions)
```

We can see that the Random Forest model and the combined model results are close to the original values. Let's verify this by checking the accuracy of the four models:
```{r accuracy, echo=TRUE, message=FALSE, warning=FALSE}
accuracy<-data.frame(confusionMatrix(pred_rf, intesting$classe)$overall[1],
                     confusionMatrix(pred_rpart, intesting$classe)$overall[1],
                     confusionMatrix(pred_lda, intesting$classe)$overall[1],
                     confusionMatrix(pred_combined, intesting$classe)$overall[1])
colnames(accuracy)<-names[-5]
print(accuracy)
```

We can see that the Random Forest model and the combined (stacked) model have very high accuraccies. 
In this case we select the Random Forest model, because it has the highest accuracy. The stacked model is more complicated and doesn't improve the accuracy, so we will better not use it.

# Prediction
Finally we can get our final predictions using the selected Random Forest model.
```{r prediction, echo=TRUE, message=FALSE, warning=FALSE}
testing_final<-testing[,subs] # subsetting the data 
 # verifying that the testing and final datasets have the same levels:
for (i in 1:ncol(testing_final)){
  levels(testing_final[,i])<-levels(dataset[,i])}
finpred_rf<-predict(mod_rf, newdata = testing_final)
```

Here are our final predictions:
```{r final, echo=TRUE, message=FALSE, warning=FALSE}
print(finpred_rf)
```

